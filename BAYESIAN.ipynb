{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "WITH DATASET\n",
        "\n",
        "import csv\n",
        "import math\n",
        "\n",
        "def load_csv(filename):\n",
        "    dataset = []\n",
        "    with open(filename, 'r') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        next(csv_reader)\n",
        "        for row in csv_reader:\n",
        "            if row:\n",
        "                dataset.append(row)\n",
        "    return dataset\n",
        "\n",
        "def prior_probabilities(labels, classes):\n",
        "    priors = {}\n",
        "    total = len(labels)\n",
        "    for c in classes:\n",
        "        priors[c] = labels.count(c) / total\n",
        "    return priors\n",
        "\n",
        "def likelihoods(features, labels, classes):\n",
        "    likelihood = {}\n",
        "    for c in classes:\n",
        "        likelihood[c] = {}\n",
        "        subset = [features[i] for i in range(len(labels)) if labels[i] == c]\n",
        "        for col in range(len(features[0])):\n",
        "            values = [row[col] for row in subset]\n",
        "            unique_vals = set(values)\n",
        "            likelihood[c][col] = {}\n",
        "            for val in unique_vals:\n",
        "                likelihood[c][col][val] = values.count(val) / len(values)\n",
        "    return likelihood\n",
        "\n",
        "def predict(X, priors, likelihood, classes):\n",
        "    results = {}\n",
        "    for c in classes:\n",
        "        prob = math.log(priors[c])\n",
        "        for i in range(len(X)):\n",
        "            if X[i] in likelihood[c][i]:\n",
        "                prob += math.log(likelihood[c][i][X[i]])\n",
        "            else:\n",
        "                prob += math.log(1e-6)\n",
        "        results[c] = prob\n",
        "    return max(results, key=results.get)\n",
        "\n",
        "filename = input(\"Enter the CSV file name (with .csv extension): \")\n",
        "\n",
        "try:\n",
        "    dataset = load_csv(filename)\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ File not found. Please make sure the file is in the same folder.\")\n",
        "    exit()\n",
        "\n",
        "features = [row[:-1] for row in dataset]\n",
        "labels = [row[-1] for row in dataset]\n",
        "classes = list(set(labels))\n",
        "\n",
        "priors = prior_probabilities(labels, classes)\n",
        "likelihood = likelihoods(features, labels, classes)\n",
        "\n",
        "predictions = [predict(row, priors, likelihood, classes) for row in features]\n",
        "\n",
        "tp = sum(1 for i in range(len(labels)) if labels[i] == 'Yes' and predictions[i] == 'Yes')\n",
        "tn = sum(1 for i in range(len(labels)) if labels[i] == 'No' and predictions[i] == 'No')\n",
        "fp = sum(1 for i in range(len(labels)) if labels[i] == 'No' and predictions[i] == 'Yes')\n",
        "fn = sum(1 for i in range(len(labels)) if labels[i] == 'Yes' and predictions[i] == 'No')\n",
        "\n",
        "accuracy = (tp + tn) / len(labels)\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "print(\"\\nPredictions vs Actual:\")\n",
        "for i in range(len(labels)):\n",
        "    print(f\"Row {i+1}: Predicted = {predictions[i]} | Actual = {labels[i]}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Model Evaluation Metrics:\")\n",
        "print(f\"Accuracy : {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall   : {recall * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nðŸ§© Confusion Matrix:\")\n",
        "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")"
      ],
      "metadata": {
        "id": "EFbrXt5P217p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "dataset = [\n",
        "    ['Sunny', 'Hot', 'High', 'Weak', 'No'],\n",
        "    ['Sunny', 'Hot', 'High', 'Strong', 'No'],\n",
        "    ['Overcast', 'Hot', 'High', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Mild', 'High', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Cool', 'Normal', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Cool', 'Normal', 'Strong', 'No'],\n",
        "    ['Overcast', 'Cool', 'Normal', 'Strong', 'Yes'],\n",
        "    ['Sunny', 'Mild', 'High', 'Weak', 'No'],\n",
        "    ['Sunny', 'Cool', 'Normal', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Mild', 'Normal', 'Weak', 'Yes'],\n",
        "    ['Sunny', 'Mild', 'Normal', 'Strong', 'Yes'],\n",
        "    ['Overcast', 'Mild', 'High', 'Strong', 'Yes'],\n",
        "    ['Overcast', 'Hot', 'Normal', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Mild', 'High', 'Strong', 'No']\n",
        "]\n",
        "\n",
        "features = [row[:-1] for row in dataset]\n",
        "labels = [row[-1] for row in dataset]\n",
        "classes = list(set(labels))\n",
        "\n",
        "def prior_probabilities(labels):\n",
        "    priors = {}\n",
        "    total = len(labels)\n",
        "    for c in classes:\n",
        "        priors[c] = labels.count(c) / total\n",
        "    return priors\n",
        "\n",
        "def likelihoods(features, labels):\n",
        "    likelihood = {}\n",
        "    for c in classes:\n",
        "        likelihood[c] = {}\n",
        "        subset = [features[i] for i in range(len(labels)) if labels[i] == c]\n",
        "        for col in range(len(features[0])):\n",
        "            values = [row[col] for row in subset]\n",
        "            unique_vals = set(values)\n",
        "            likelihood[c][col] = {}\n",
        "            for val in unique_vals:\n",
        "                likelihood[c][col][val] = values.count(val) / len(values)\n",
        "    return likelihood\n",
        "\n",
        "def predict(X, priors, likelihood):\n",
        "    results = {}\n",
        "    for c in classes:\n",
        "        prob = math.log(priors[c])\n",
        "        for i in range(len(X)):\n",
        "            if X[i] in likelihood[c][i]:\n",
        "                prob += math.log(likelihood[c][i][X[i]])\n",
        "            else:\n",
        "                prob += math.log(1e-6)\n",
        "        results[c] = prob\n",
        "    return max(results, key=results.get)\n",
        "\n",
        "priors = prior_probabilities(labels)\n",
        "likelihood = likelihoods(features, labels)\n",
        "\n",
        "predictions = []\n",
        "for row in features:\n",
        "    pred = predict(row, priors, likelihood)\n",
        "    predictions.append(pred)\n",
        "\n",
        "tp = sum(1 for i in range(len(labels)) if labels[i] == 'Yes' and predictions[i] == 'Yes')\n",
        "tn = sum(1 for i in range(len(labels)) if labels[i] == 'No' and predictions[i] == 'No')\n",
        "fp = sum(1 for i in range(len(labels)) if labels[i] == 'No' and predictions[i] == 'Yes')\n",
        "fn = sum(1 for i in range(len(labels)) if labels[i] == 'Yes' and predictions[i] == 'No')\n",
        "\n",
        "accuracy = (tp + tn) / len(labels)\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"Actual:\", labels)\n",
        "print(\"\\nModel Evaluation Metrics:\")\n",
        "print(f\"Accuracy : {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall   : {recall * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "41mk6GqF3IvQ",
        "outputId": "9ead42e0-3924-45f3-aaa2-505ab8f68d74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: ['No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
            "Actual: ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
            "\n",
            "Model Evaluation Metrics:\n",
            "Accuracy : 92.86%\n",
            "Precision: 90.00%\n",
            "Recall   : 100.00%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}