{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import graphviz\n",
        "\n",
        "def calculate_entropy(data, target_attribute):\n",
        "    total_samples = len(data)\n",
        "    entropy = 0.0\n",
        "    value_counts = data[target_attribute].value_counts()\n",
        "    for count in value_counts:\n",
        "        probability = count / total_samples\n",
        "        if probability > 0:\n",
        "            entropy -= probability * math.log2(probability)\n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(data, feature_attribute, target_attribute):\n",
        "    initial_entropy = calculate_entropy(data, target_attribute)\n",
        "    total_samples = len(data)\n",
        "    weighted_entropy_after_split = 0.0\n",
        "    for value in data[feature_attribute].unique():\n",
        "        subset = data[data[feature_attribute] == value]\n",
        "        subset_entropy = calculate_entropy(subset, target_attribute)\n",
        "        weight = len(subset) / total_samples\n",
        "        weighted_entropy_after_split += weight * subset_entropy\n",
        "    information_gain = initial_entropy - weighted_entropy_after_split\n",
        "    return information_gain\n",
        "\n",
        "def train_id3_and_visualize(df, target_column, feature_columns):\n",
        "    le = LabelEncoder()\n",
        "    df_encoded = df[feature_columns + [target_column]].copy()\n",
        "    for column in feature_columns + [target_column]:\n",
        "        if df_encoded[column].dtype == 'object':\n",
        "            df_encoded[column] = le.fit_transform(df_encoded[column])\n",
        "\n",
        "    features = df_encoded[feature_columns]\n",
        "    target = df_encoded[target_column]\n",
        "\n",
        "    id3_classifier = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
        "    id3_classifier.fit(features, target)\n",
        "\n",
        "    dot_data = export_graphviz(\n",
        "        id3_classifier,\n",
        "        out_file=None,\n",
        "        feature_names=features.columns,\n",
        "        class_names=[str(c) for c in id3_classifier.classes_],\n",
        "        filled=True,\n",
        "        rounded=True,\n",
        "        special_characters=True\n",
        "    )\n",
        "    graph = graphviz.Source(dot_data)\n",
        "    graph.render(\"id3_tree\", format=\"png\", cleanup=True)\n",
        "    print(\"\\n✅ Decision tree visualization saved as 'id3_tree.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== ID3 Decision Tree ===\\n\")\n",
        "\n",
        "    num_rows = int(input(\"Enter number of data samples (rows): \"))\n",
        "    feature_columns = input(\"Enter feature names (comma-separated): \").strip().split(',')\n",
        "    target_column = input(\"Enter target column name: \").strip()\n",
        "\n",
        "    all_columns = feature_columns + [target_column]\n",
        "    dataset = []\n",
        "\n",
        "    print(f\"\\nEnter the dataset rows one by one (features + target):\")\n",
        "    print(f\"For example, if columns are {all_columns}, enter values separated by commas.\\n\")\n",
        "\n",
        "    for i in range(num_rows):\n",
        "        row = input(f\"Row {i+1}: \").strip().split(',')\n",
        "        dataset.append(row)\n",
        "\n",
        "    df = pd.DataFrame(dataset, columns=all_columns)\n",
        "\n",
        "    initial_entropy = calculate_entropy(df, target_column)\n",
        "    print(f\"\\nInitial Entropy of '{target_column}': {initial_entropy:.4f}\")\n",
        "    print(\"\\nInformation Gain for each feature:\")\n",
        "    for feature in feature_columns:\n",
        "        gain = calculate_information_gain(df, feature, target_column)\n",
        "        print(f\" - {feature}: {gain:.4f}\")\n",
        "\n",
        "    print(\"\\nTraining and visualizing decision tree...\")\n",
        "    train_id3_and_visualize(df, target_column, feature_columns)\n"
      ],
      "metadata": {
        "id": "OxSRzSin38y4",
        "outputId": "5c399e76-54b3-479f-95a6-bf5e09ea12d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ID3 Decision Tree ===\n",
            "\n",
            "Enter number of data samples (rows): 4\n",
            "Enter feature names (comma-separated): outlook,temperature,humidity\n",
            "Enter target column name: PLAY\n",
            "\n",
            "Enter the dataset rows one by one (features + target):\n",
            "For example, if columns are ['outlook', 'temperature', 'humidity', 'PLAY'], enter values separated by commas.\n",
            "\n",
            "Row 1: sunny,hot,high,no\n",
            "Row 2: sunny,mild,high,no\n",
            "Row 3: overcast,hot,normal,yes\n",
            "Row 4: rainy,cool,normal,yes\n",
            "\n",
            "Initial Entropy of 'PLAY': 1.0000\n",
            "\n",
            "Information Gain for each feature:\n",
            " - outlook: 1.0000\n",
            " - temperature: 0.5000\n",
            " - humidity: 1.0000\n",
            "\n",
            "Training and visualizing decision tree...\n",
            "\n",
            "✅ Decision tree visualization saved as 'id3_tree.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_entropy(data, target_attribute):\n",
        "    total_samples = len(data)\n",
        "    entropy = 0.0\n",
        "    value_counts = data[target_attribute].value_counts()\n",
        "    for count in value_counts:\n",
        "        probability = count / total_samples\n",
        "        if probability > 0:\n",
        "            entropy -= probability * math.log2(probability)\n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(data, feature_attribute, target_attribute):\n",
        "    initial_entropy = calculate_entropy(data, target_attribute)\n",
        "    total_samples = len(data)\n",
        "    weighted_entropy_after_split = 0.0\n",
        "    for value in data[feature_attribute].unique():\n",
        "        subset = data[data[feature_attribute] == value]\n",
        "        subset_entropy = calculate_entropy(subset, target_attribute)\n",
        "        weight = len(subset) / total_samples\n",
        "        weighted_entropy_after_split += weight * subset_entropy\n",
        "    information_gain = initial_entropy - weighted_entropy_after_split\n",
        "    return information_gain\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== ID3 Entropy & Information Gain Calculator ===\\n\")\n",
        "\n",
        "    num_rows = int(input(\"Enter number of data samples (rows): \"))\n",
        "    feature_columns = input(\"Enter feature names (comma-separated): \").strip().split(',')\n",
        "    target_column = input(\"Enter target column name: \").strip()\n",
        "\n",
        "    all_columns = feature_columns + [target_column]\n",
        "    dataset = []\n",
        "\n",
        "    print(f\"\\nEnter the dataset rows one by one (features + target):\")\n",
        "    print(f\"For example, if columns are {all_columns}, enter values separated by commas.\\n\")\n",
        "\n",
        "    for i in range(num_rows):\n",
        "        row = input(f\"Row {i+1}: \").strip().split(',')\n",
        "        dataset.append(row)\n",
        "\n",
        "    df = pd.DataFrame(dataset, columns=all_columns)\n",
        "\n",
        "    initial_entropy = calculate_entropy(df, target_column)\n",
        "    print(f\"\\nInitial Entropy of '{target_column}': {initial_entropy:.4f}\")\n",
        "\n",
        "    print(\"\\nInformation Gain for each feature:\")\n",
        "    info_gains = {}\n",
        "    for feature in feature_columns:\n",
        "        gain = calculate_information_gain(df, feature, target_column)\n",
        "        info_gains[feature] = gain\n",
        "        print(f\" - {feature}: {gain:.4f}\")\n",
        "\n",
        "    best_feature = max(info_gains, key=info_gains.get)\n",
        "    print(f\"\\n The feature with the highest Information Gain is: '{best_feature}'\")\n"
      ],
      "metadata": {
        "id": "6AnwaZXN4ckx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}